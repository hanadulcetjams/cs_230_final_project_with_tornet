{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b99b3ba1-a966-4502-857f-ef0344d141be",
   "metadata": {},
   "source": [
    "# Data Augmentation\n",
    "\n",
    "The goal of this notebook is to use data augmentation to improve upon the results of the original TorNet baseline CNN model.\n",
    "\n",
    "In this notebook, we implement the image flipping used to only add flipped versions of tornado images to the TorNet dataset.\n",
    "We then take that data and re-train the original architecture of the TorNet model (albeit with our re-tuned hyperparameters\n",
    "for batch size and learning rate) in order to see whether the data augmentation strategy will improve the quality of the \n",
    "model that is trained by addressing the class imbalance due to the rarity of tornadoes in the dataset.\n",
    "\n",
    "Of note - as mentioned in the project milestone, we can't use region cropping or translation here, because the \n",
    "original data is radar data which is being measured on a slant (not top-down) and hence has a natural stretching/distortion\n",
    "that occurs depending on how far away a storm cell is from the center of the radar image. If we move the data around, \n",
    "we risk introducing our own squishing artifacts on the shapes of the cells which would poison our dataset. \n",
    "\n",
    "In comparison, mirroring here could be very helpful (without translating the data) to generate more examples of \n",
    "tornadoes without having to find more tornado radar images. Hence, we implement that strategy here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "052ae79f-b46e-47ec-b29d-4538a9233d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# just the location for the input data\n",
    "TORNET_DATA_INPUT_FOLDER = \"/mnt/c/users/handypark/Documents/Grad_School_Courses/CS_230/tornet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e4063f0-8bf4-48f6-ad78-b602b3216db1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ngpus = tf.config.list_physical_devices(\\'GPU\\')\\nif gpus:\\n  try:\\n    # Currently, memory growth needs to be the same across GPUs\\n    for gpu in gpus:\\n      tf.config.experimental.set_memory_growth(gpu, True)\\n    logical_gpus = tf.config.list_logical_devices(\\'GPU\\')\\n    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\\n  except RuntimeError as e:\\n    # Memory growth must be set before GPUs have been initialized\\n    print(e)\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Just making sure that we are indeed using GPU-based Tensorflow and not CPU-based Tensorflow.\n",
    "tf.test.is_built_with_cuda()\n",
    "\n",
    "# We tried experimental memory growth in some cases, but it didn't work out well (also crashed a lot).\n",
    "\"\"\"\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677cb397-68c6-47a7-8a18-577995858d9e",
   "metadata": {},
   "source": [
    "## Creating a dataset with additional mirrored tornado examples\n",
    "\n",
    "This time around, now that we have the learning rate and batch size all tuned, we can start trying to improve the performance of the model.\n",
    "Our first idea is to try using data augmentation. Since examples of tornados are rare (and comprise only a tiny portion of the data),\n",
    "the hope here is to try to artificially increase the number of examples of tornadoes to help correct this class imbalance.\n",
    "\n",
    "The initial paper used weights on different kinds of examples, but the hope here is that augmenting the data can be even more effective.\n",
    "What we've done below is to take TorNet's dataset loading + creation code and introduce a flipping step, where we do the following:\n",
    "- Check if the given training set image is a tornadic example.\n",
    "- If not, proceed as normal.\n",
    "- If so, add another copy of the image that we have flipped to the training set.\n",
    "\n",
    "With this data augmentation, we're hoping to see some improvement in the performance of the model on the testing data in terms of AUC, just from the additional \"examples\" of tornados that we get a result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "59daf357-3d58-45a9-a8fd-4f8062907a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "TorNet's data loading code, but now with some adjustments for data augmentation purposes.\n",
    "We create `create_tf_with_flips_dataset` in order to make a dataset that has both\n",
    "tornadic examples and mirrored tornadic examples.\n",
    "\"\"\"\n",
    "from typing import List, Dict\n",
    "\n",
    "from tornet.data.loader import query_catalog, read_file\n",
    "from tornet.data.constants import ALL_VARIABLES\n",
    "from tornet.data import preprocess as pp\n",
    "import numpy as np\n",
    "\n",
    "def create_tf_with_flips_dataset(files:str,\n",
    "                                 variables: List[str]=ALL_VARIABLES,\n",
    "                                 n_frames:int=1,\n",
    "                                 tilt_last: bool=True) -> tf.data.Dataset:\n",
    "    \"\"\"\n",
    "    This is Tornet's main function for loading the data from the folder where it's all stored, but we've \n",
    "    made adjustments to actually add a system for flipping tornadic images.\n",
    "\n",
    "    In this case, we add non-tornadic examples as normal, but when we encounter an example of\n",
    "    a tornado, we actually add both the original tornadic example and a mirrored version\n",
    "    of the tornadic example to the dataset.\n",
    "    \"\"\"\n",
    "    assert len(files)>0\n",
    "    \n",
    "    # grab one file to gets keys, shapes, etc\n",
    "    data = read_file(files[0],variables=variables,n_frames=n_frames, tilt_last=tilt_last)\n",
    "    \n",
    "    output_signature = { k:tf.TensorSpec(shape=data[k].shape,dtype=data[k].dtype,name=k) for k in data }\n",
    "\n",
    "    # Iterate through all the files and keep track of which are tornadic examples, and add \n",
    "    # a \"to-be-flipped\" copy for each case of a tornado.\n",
    "    file_list = []\n",
    "    for f in files:\n",
    "        file_list.append((f, 0))\n",
    "        file_dict = read_file(f,variables=variables,n_frames=n_frames, tilt_last=tilt_last)\n",
    "        if file_dict[\"label\"] == 1:\n",
    "            file_list.append((f, 1))\n",
    "\n",
    "    # When generating the data, check to see whether we should flip the data,\n",
    "    # then flip if necessary.\n",
    "    def gen():\n",
    "        for (f, flipper) in file_list:\n",
    "            file_dict = read_file(f,variables=variables,n_frames=n_frames, tilt_last=tilt_last)\n",
    "            if flipper == 1:\n",
    "                # Each of the 6 different radar images gets flipped using np.flip\n",
    "                for var in ['DBZ','VEL','KDP','RHOHV','ZDR','WIDTH']:\n",
    "                    file_dict[var] = tf.convert_to_tensor(np.flip(file_dict[var], 1))\n",
    "            yield file_dict\n",
    "    ds = tf.data.Dataset.from_generator(gen,\n",
    "                                        output_signature=output_signature)\n",
    "    return ds\n",
    "    \n",
    "\n",
    "def preproc(ds: tf.data.Dataset,\n",
    "            weights:Dict=None,\n",
    "            include_az:bool=False,\n",
    "            select_keys:list=None,\n",
    "            tilt_last:bool=True):\n",
    "    \"\"\"\n",
    "    This is Tornet's preprocessing function, unchanged,\n",
    "    for taking the raw dataset loaded from the files (in create_tf_dataset)\n",
    "    and then doing a few things:\n",
    "\n",
    "    - Remove the time dimension (since we only care about detection at a given time t)\n",
    "    - Add coordinates (so that we can run CoordConv layers later)\n",
    "    - Split the data into its inputs and label outputs\n",
    "    - Adding weights (if we decide to weight the data at all)\n",
    "\n",
    "    Once the preprocessing is done, the data is basically ready to be trained on.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Remove time dimension\n",
    "    ds = ds.map(pp.remove_time_dim)\n",
    "\n",
    "    # Add coordinate tensors\n",
    "    ds = ds.map(lambda d: pp.add_coordinates(d,include_az=include_az,tilt_last=tilt_last,backend=tf))\n",
    "\n",
    "    # split into X,y\n",
    "    ds = ds.map(pp.split_x_y)\n",
    "\n",
    "    # Add sample weights\n",
    "    if weights:\n",
    "        ds = ds.map(lambda x,y:  pp.compute_sample_weight(x,y,**weights, backend=tf) )\n",
    "    \n",
    "        # select keys for input\n",
    "        if select_keys is not None:\n",
    "            ds = ds.map(lambda x,y,w: (pp.select_keys(x,keys=select_keys),y,w))\n",
    "    else:\n",
    "        if select_keys is not None:\n",
    "            ds = ds.map(lambda x,y: (pp.select_keys(x,keys=select_keys),y))\n",
    "\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "57fd9150-7c8c-4cea-bd85-4b862ad28639",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tf_with_flips_loader(data_root: str, \n",
    "                              data_type:str='train', # or 'test'\n",
    "                              years: list=list(range(2013,2023)),\n",
    "                              batch_size: int=128, \n",
    "                              weights: Dict=None,\n",
    "                              include_az: bool=False,\n",
    "                              random_state:int=1234,\n",
    "                              select_keys: list=None,\n",
    "                              tilt_last: bool=True,\n",
    "                              from_tfds: bool=False,\n",
    "                              tfds_data_version: str='1.1.0',\n",
    "                              num_epochs: int=3):\n",
    "    \"\"\"\n",
    "    We've trimmed down the make_tf_loader function from the TorNet helper\n",
    "    functions to make clear which part of the loader we're using.\n",
    "\n",
    "    Now that we have a function for making a dataset with flipped tornado\n",
    "    examples added in, we make use of it here.\n",
    "    \"\"\"\n",
    "    # get files from the catalog\n",
    "    file_list = query_catalog(data_root, data_type, years, random_state)\n",
    "\n",
    "    # make the dataset using the flipped data generator\n",
    "    ds = create_tf_with_flips_dataset(file_list,variables=ALL_VARIABLES,n_frames=1, tilt_last=tilt_last) \n",
    "\n",
    "    # do the usual preprocessing to prepare the input for Tensorflow training\n",
    "    ds = preproc(ds,weights,include_az,select_keys,tilt_last)\n",
    "    ds = ds.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    # this has been adjusted to include drop_remainder=True\n",
    "    ds = ds.batch(batch_size, drop_remainder=True)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6414f3-a24c-4242-9c0a-0488d29b12ad",
   "metadata": {},
   "source": [
    "## TorNet Baseline CNN Model Definition:\n",
    "\n",
    "As stated in the comment for the code block below, this is just the TorNet model code that was used in the original paper.\n",
    "We'll be using the same model for now during our data augmentation experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8d7d2e8a-6fdf-4017-a395-7a9fa08d8e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This is just the TorNet model code that was used in the paper.\n",
    "Goal is to run this model which consists of:\n",
    "- Normalizing the inputs\n",
    "- Adding the coordinate information for CoordConv to work properly\n",
    "- Running 4 VGG blocks which each have CoordConv2D and two MAXPOOL layers\n",
    "- One last block of Conv2D layers and MAXPOOL to get the output probability\n",
    "\"\"\"\n",
    "\n",
    "from typing import Dict, List, Tuple\n",
    "import numpy as np\n",
    "import keras\n",
    "from tornet.models.keras.layers import CoordConv2D, FillNaNs\n",
    "from tornet.data.constants import CHANNEL_MIN_MAX, ALL_VARIABLES\n",
    "\n",
    "\n",
    "def build_model(shape:Tuple[int]=(120,240,2),\n",
    "                c_shape:Tuple[int]=(120,240,2),\n",
    "                input_variables:List[str]=ALL_VARIABLES,\n",
    "                start_filters:int=64,\n",
    "                l2_reg:float=0.001,\n",
    "                background_flag:float=-3.0,\n",
    "                include_range_folded:bool=True,\n",
    "                head='maxpool'):\n",
    "    # Create input layers for each input_variables\n",
    "    inputs = {}\n",
    "    for v in input_variables:\n",
    "        inputs[v]=keras.Input(shape,name=v)\n",
    "    n_sweeps=shape[2]\n",
    "    \n",
    "    # Normalize inputs and concate along channel dim\n",
    "    normalized_inputs=keras.layers.Concatenate(axis=-1,name='Concatenate1')(\n",
    "        [normalize(inputs[v],v) for v in input_variables]\n",
    "        )\n",
    "\n",
    "    # Replace nan pixel with background flag\n",
    "    normalized_inputs = FillNaNs(background_flag)(normalized_inputs)\n",
    "\n",
    "    # Add channel for range folded gates \n",
    "    if include_range_folded:\n",
    "        range_folded = keras.Input(shape[:2]+(n_sweeps,),name='range_folded_mask')\n",
    "        inputs['range_folded_mask']=range_folded\n",
    "        normalized_inputs = keras.layers.Concatenate(axis=-1,name='Concatenate2')(\n",
    "               [normalized_inputs,range_folded])\n",
    "        \n",
    "    # Input coordinate information\n",
    "    cin=keras.Input(c_shape,name='coordinates')\n",
    "    inputs['coordinates']=cin\n",
    "\n",
    "    x,c = normalized_inputs,cin\n",
    "    \n",
    "    x,c = vgg_block(x,c, filters=start_filters,   ksize=3, l2_reg=l2_reg, n_convs=2, drop_rate=0.1)   # (60,120)\n",
    "    x,c = vgg_block(x,c, filters=2*start_filters, ksize=3, l2_reg=l2_reg, n_convs=2, drop_rate=0.1)  # (30,60)\n",
    "    x,c = vgg_block(x,c, filters=4*start_filters, ksize=3, l2_reg=l2_reg, n_convs=3, drop_rate=0.1)  # (15,30)\n",
    "    x,c = vgg_block(x,c, filters=8*start_filters, ksize=3, l2_reg=l2_reg, n_convs=3, drop_rate=0.1)  # (7,15)\n",
    "    #x,c = vgg_block(x,c, filters=8*start_filters, ksize=3, l2_reg=l2_reg, n_convs=3)  # (3,7)\n",
    "    \n",
    "    if head=='mlp':\n",
    "        # MLP head\n",
    "        x = keras.layers.Flatten()(x) \n",
    "        x = keras.layers.Dense(units = 4096, activation ='relu')(x) \n",
    "        x = keras.layers.Dense(units = 2024, activation ='relu')(x) \n",
    "        output = keras.layers.Dense(1)(x)\n",
    "    elif head=='maxpool':\n",
    "        # Per gridcell\n",
    "        x = keras.layers.Conv2D(filters=512, kernel_size=1,\n",
    "                          kernel_regularizer=keras.regularizers.l2(l2_reg),\n",
    "                          activation='relu')(x)\n",
    "        x = keras.layers.Conv2D(filters=256, kernel_size=1,\n",
    "                          kernel_regularizer=keras.regularizers.l2(l2_reg),\n",
    "                          activation='relu')(x)\n",
    "        x = keras.layers.Conv2D(filters=1, kernel_size=1,name='heatmap')(x)\n",
    "        # Max in scene\n",
    "        output = keras.layers.GlobalMaxPooling2D()(x)\n",
    "\n",
    "    return keras.Model(inputs=inputs,outputs=output)\n",
    "\n",
    "\n",
    "def vgg_block(x,c, filters=64, ksize=3, n_convs=2, l2_reg=1e-6, drop_rate=0.0):\n",
    "\n",
    "    for _ in range(n_convs):\n",
    "        x,c = CoordConv2D(filters=filters,\n",
    "                          kernel_size=ksize,\n",
    "                          kernel_regularizer=keras.regularizers.l2(l2_reg),\n",
    "                          padding='same',\n",
    "                          activation='relu')([x,c])\n",
    "    x = keras.layers.MaxPool2D(pool_size =2, strides =2, padding ='same')(x)\n",
    "    c = keras.layers.MaxPool2D(pool_size =2, strides =2, padding ='same')(c)\n",
    "    if drop_rate>0:\n",
    "        x = keras.layers.Dropout(rate=drop_rate)(x)\n",
    "    return x,c\n",
    "\n",
    "\n",
    "def normalize(x,\n",
    "              name:str):\n",
    "    \"\"\"\n",
    "    Channel-wise normalization using known CHANNEL_MIN_MAX\n",
    "    \"\"\"\n",
    "    min_max = np.array(CHANNEL_MIN_MAX[name]) # [2,]\n",
    "    n_sweeps=x.shape[-1]\n",
    "    \n",
    "    # choose mean,var to get approximate [-1,1] scaling\n",
    "    var=((min_max[1]-min_max[0])/2)**2 # scalar\n",
    "    var=np.array(n_sweeps*[var,])    # [n_sweeps,]\n",
    "    \n",
    "    offset=(min_max[0]+min_max[1])/2    # scalar\n",
    "    offset=np.array(n_sweeps*[offset,]) # [n_sweeps,]\n",
    "\n",
    "    return keras.layers.Normalization(mean=offset,\n",
    "                                      variance=var,\n",
    "                                      name='Normalize_%s' % name)(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8144dffa-e578-423e-8e11-53b6fb6ce4cb",
   "metadata": {},
   "source": [
    "## Running the Model\n",
    "\n",
    "Ok, we've got all of the TorNet model components figured out and imported (using the helper functions, etc.).\n",
    "[Even getting that working turned out to be tricky - there were dependency issues to resolve, and while we'd \n",
    "like to just be able to import the functions rather than re-copying them here again, that wasn't working so well.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8f9c3be5-ebd6-4c7c-83ac-bf8708944ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "37672757-38a1-4e97-a135-c527e7ae02a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ DBZ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">240</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ VEL (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">240</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ KDP (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">240</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ RHOHV (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">240</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ ZDR (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">240</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ WIDTH (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">240</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ Normalize_DBZ       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">240</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ DBZ[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Normalization</span>)     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ Normalize_VEL       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">240</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ VEL[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Normalization</span>)     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ Normalize_KDP       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">240</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ KDP[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Normalization</span>)     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ Normalize_RHOHV     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">240</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ RHOHV[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Normalization</span>)     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ Normalize_ZDR       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">240</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ ZDR[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Normalization</span>)     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ Normalize_WIDTH     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">240</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ WIDTH[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Normalization</span>)     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ Concatenate1        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">240</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ Normalize_DBZ[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │ <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>)               │            │ Normalize_VEL[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│                     │                   │            │ Normalize_KDP[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│                     │                   │            │ Normalize_RHOHV[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│                     │                   │            │ Normalize_ZDR[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│                     │                   │            │ Normalize_WIDTH[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ isnan_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Isnan</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">240</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ Concatenate1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ where_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Where</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">240</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ isnan_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],    │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>)               │            │ Concatenate1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ range_folded_mask   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">240</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ Concatenate2        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">240</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ where_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │ <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>)               │            │ range_folded_mas… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ coordinates         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">240</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ coord_conv2d_10     │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">240</span>, │      <span style=\"color: #00af00; text-decoration-color: #00af00\">9,280</span> │ Concatenate2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">CoordConv2D</span>)       │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>,  │            │ coordinates[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">240</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)]          │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ coord_conv2d_11     │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">240</span>, │     <span style=\"color: #00af00; text-decoration-color: #00af00\">38,080</span> │ coord_conv2d_10[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">CoordConv2D</span>)       │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>,  │            │ coord_conv2d_10[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">240</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)]          │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling2d_8     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>,   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ coord_conv2d_11[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>,   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ max_pooling2d_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling2d_9     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>,   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ coord_conv2d_11[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ coord_conv2d_12     │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>,  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">76,160</span> │ dropout_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">CoordConv2D</span>)       │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>,  │            │ max_pooling2d_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)]          │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ coord_conv2d_13     │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>,  │    <span style=\"color: #00af00; text-decoration-color: #00af00\">149,888</span> │ coord_conv2d_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">CoordConv2D</span>)       │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>,  │            │ coord_conv2d_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)]          │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling2d_10    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ coord_conv2d_13[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ max_pooling2d_10… │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling2d_11    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ coord_conv2d_13[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ coord_conv2d_14     │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>,   │    <span style=\"color: #00af00; text-decoration-color: #00af00\">299,776</span> │ dropout_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">CoordConv2D</span>)       │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>,  │            │ max_pooling2d_11… │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)]           │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ coord_conv2d_15     │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>,   │    <span style=\"color: #00af00; text-decoration-color: #00af00\">594,688</span> │ coord_conv2d_14[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">CoordConv2D</span>)       │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>,  │            │ coord_conv2d_14[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)]           │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ coord_conv2d_16     │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>,   │    <span style=\"color: #00af00; text-decoration-color: #00af00\">594,688</span> │ coord_conv2d_15[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">CoordConv2D</span>)       │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>,  │            │ coord_conv2d_15[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)]           │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling2d_12    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ coord_conv2d_16[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ max_pooling2d_12… │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling2d_13    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ coord_conv2d_16[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ coord_conv2d_17     │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>,   │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,189,376</span> │ dropout_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">CoordConv2D</span>)       │ <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>,  │            │ max_pooling2d_13… │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)]           │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ coord_conv2d_18     │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>,   │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,369,024</span> │ coord_conv2d_17[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">CoordConv2D</span>)       │ <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>,  │            │ coord_conv2d_17[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)]           │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ coord_conv2d_19     │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>,   │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,369,024</span> │ coord_conv2d_18[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">CoordConv2D</span>)       │ <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>,  │            │ coord_conv2d_18[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)]           │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling2d_14    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>,     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ coord_conv2d_19[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>,     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ max_pooling2d_14… │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>,     │    <span style=\"color: #00af00; text-decoration-color: #00af00\">262,656</span> │ dropout_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_23 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>,     │    <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │ conv2d_22[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ heatmap (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">257</span> │ conv2d_23[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_max_pooling… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ heatmap[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling2…</span> │                   │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ DBZ (\u001b[38;5;33mInputLayer\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m120\u001b[0m, \u001b[38;5;34m240\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│                     │ \u001b[38;5;34m2\u001b[0m)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ VEL (\u001b[38;5;33mInputLayer\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m120\u001b[0m, \u001b[38;5;34m240\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│                     │ \u001b[38;5;34m2\u001b[0m)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ KDP (\u001b[38;5;33mInputLayer\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m120\u001b[0m, \u001b[38;5;34m240\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│                     │ \u001b[38;5;34m2\u001b[0m)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ RHOHV (\u001b[38;5;33mInputLayer\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m120\u001b[0m, \u001b[38;5;34m240\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│                     │ \u001b[38;5;34m2\u001b[0m)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ ZDR (\u001b[38;5;33mInputLayer\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m120\u001b[0m, \u001b[38;5;34m240\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│                     │ \u001b[38;5;34m2\u001b[0m)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ WIDTH (\u001b[38;5;33mInputLayer\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m120\u001b[0m, \u001b[38;5;34m240\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│                     │ \u001b[38;5;34m2\u001b[0m)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ Normalize_DBZ       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m120\u001b[0m, \u001b[38;5;34m240\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ DBZ[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
       "│ (\u001b[38;5;33mNormalization\u001b[0m)     │ \u001b[38;5;34m2\u001b[0m)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ Normalize_VEL       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m120\u001b[0m, \u001b[38;5;34m240\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ VEL[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
       "│ (\u001b[38;5;33mNormalization\u001b[0m)     │ \u001b[38;5;34m2\u001b[0m)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ Normalize_KDP       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m120\u001b[0m, \u001b[38;5;34m240\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ KDP[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
       "│ (\u001b[38;5;33mNormalization\u001b[0m)     │ \u001b[38;5;34m2\u001b[0m)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ Normalize_RHOHV     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m120\u001b[0m, \u001b[38;5;34m240\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ RHOHV[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "│ (\u001b[38;5;33mNormalization\u001b[0m)     │ \u001b[38;5;34m2\u001b[0m)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ Normalize_ZDR       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m120\u001b[0m, \u001b[38;5;34m240\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ ZDR[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
       "│ (\u001b[38;5;33mNormalization\u001b[0m)     │ \u001b[38;5;34m2\u001b[0m)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ Normalize_WIDTH     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m120\u001b[0m, \u001b[38;5;34m240\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ WIDTH[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "│ (\u001b[38;5;33mNormalization\u001b[0m)     │ \u001b[38;5;34m2\u001b[0m)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ Concatenate1        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m120\u001b[0m, \u001b[38;5;34m240\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ Normalize_DBZ[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │ \u001b[38;5;34m12\u001b[0m)               │            │ Normalize_VEL[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│                     │                   │            │ Normalize_KDP[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│                     │                   │            │ Normalize_RHOHV[\u001b[38;5;34m…\u001b[0m │\n",
       "│                     │                   │            │ Normalize_ZDR[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│                     │                   │            │ Normalize_WIDTH[\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ isnan_1 (\u001b[38;5;33mIsnan\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m120\u001b[0m, \u001b[38;5;34m240\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ Concatenate1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "│                     │ \u001b[38;5;34m12\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ where_1 (\u001b[38;5;33mWhere\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m120\u001b[0m, \u001b[38;5;34m240\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ isnan_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],    │\n",
       "│                     │ \u001b[38;5;34m12\u001b[0m)               │            │ Concatenate1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ range_folded_mask   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m120\u001b[0m, \u001b[38;5;34m240\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │ \u001b[38;5;34m2\u001b[0m)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ Concatenate2        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m120\u001b[0m, \u001b[38;5;34m240\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ where_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],    │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │ \u001b[38;5;34m14\u001b[0m)               │            │ range_folded_mas… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ coordinates         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m120\u001b[0m, \u001b[38;5;34m240\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │ \u001b[38;5;34m2\u001b[0m)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ coord_conv2d_10     │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m120\u001b[0m, \u001b[38;5;34m240\u001b[0m, │      \u001b[38;5;34m9,280\u001b[0m │ Concatenate2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mCoordConv2D\u001b[0m)       │ \u001b[38;5;34m64\u001b[0m), (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m120\u001b[0m,  │            │ coordinates[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "│                     │ \u001b[38;5;34m240\u001b[0m, \u001b[38;5;34m2\u001b[0m)]          │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ coord_conv2d_11     │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m120\u001b[0m, \u001b[38;5;34m240\u001b[0m, │     \u001b[38;5;34m38,080\u001b[0m │ coord_conv2d_10[\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mCoordConv2D\u001b[0m)       │ \u001b[38;5;34m64\u001b[0m), (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m120\u001b[0m,  │            │ coord_conv2d_10[\u001b[38;5;34m…\u001b[0m │\n",
       "│                     │ \u001b[38;5;34m240\u001b[0m, \u001b[38;5;34m2\u001b[0m)]          │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling2d_8     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m120\u001b[0m,   │          \u001b[38;5;34m0\u001b[0m │ coord_conv2d_11[\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m120\u001b[0m,   │          \u001b[38;5;34m0\u001b[0m │ max_pooling2d_8[\u001b[38;5;34m…\u001b[0m │\n",
       "│                     │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling2d_9     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m120\u001b[0m,   │          \u001b[38;5;34m0\u001b[0m │ coord_conv2d_11[\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ \u001b[38;5;34m2\u001b[0m)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ coord_conv2d_12     │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m120\u001b[0m,  │     \u001b[38;5;34m76,160\u001b[0m │ dropout_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│ (\u001b[38;5;33mCoordConv2D\u001b[0m)       │ \u001b[38;5;34m128\u001b[0m), (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m,  │            │ max_pooling2d_9[\u001b[38;5;34m…\u001b[0m │\n",
       "│                     │ \u001b[38;5;34m120\u001b[0m, \u001b[38;5;34m2\u001b[0m)]          │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ coord_conv2d_13     │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m120\u001b[0m,  │    \u001b[38;5;34m149,888\u001b[0m │ coord_conv2d_12[\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mCoordConv2D\u001b[0m)       │ \u001b[38;5;34m128\u001b[0m), (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m,  │            │ coord_conv2d_12[\u001b[38;5;34m…\u001b[0m │\n",
       "│                     │ \u001b[38;5;34m120\u001b[0m, \u001b[38;5;34m2\u001b[0m)]          │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling2d_10    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m60\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ coord_conv2d_13[\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ \u001b[38;5;34m128\u001b[0m)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m60\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ max_pooling2d_10… │\n",
       "│                     │ \u001b[38;5;34m128\u001b[0m)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling2d_11    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m2\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ coord_conv2d_13[\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ coord_conv2d_14     │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m60\u001b[0m,   │    \u001b[38;5;34m299,776\u001b[0m │ dropout_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│ (\u001b[38;5;33mCoordConv2D\u001b[0m)       │ \u001b[38;5;34m256\u001b[0m), (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m,  │            │ max_pooling2d_11… │\n",
       "│                     │ \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m2\u001b[0m)]           │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ coord_conv2d_15     │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m60\u001b[0m,   │    \u001b[38;5;34m594,688\u001b[0m │ coord_conv2d_14[\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mCoordConv2D\u001b[0m)       │ \u001b[38;5;34m256\u001b[0m), (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m,  │            │ coord_conv2d_14[\u001b[38;5;34m…\u001b[0m │\n",
       "│                     │ \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m2\u001b[0m)]           │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ coord_conv2d_16     │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m60\u001b[0m,   │    \u001b[38;5;34m594,688\u001b[0m │ coord_conv2d_15[\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mCoordConv2D\u001b[0m)       │ \u001b[38;5;34m256\u001b[0m), (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m,  │            │ coord_conv2d_15[\u001b[38;5;34m…\u001b[0m │\n",
       "│                     │ \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m2\u001b[0m)]           │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling2d_12    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m30\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ coord_conv2d_16[\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ \u001b[38;5;34m256\u001b[0m)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_6 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m30\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ max_pooling2d_12… │\n",
       "│                     │ \u001b[38;5;34m256\u001b[0m)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling2d_13    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m2\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ coord_conv2d_16[\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ coord_conv2d_17     │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m30\u001b[0m,   │  \u001b[38;5;34m1,189,376\u001b[0m │ dropout_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│ (\u001b[38;5;33mCoordConv2D\u001b[0m)       │ \u001b[38;5;34m512\u001b[0m), (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m,  │            │ max_pooling2d_13… │\n",
       "│                     │ \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m2\u001b[0m)]           │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ coord_conv2d_18     │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m30\u001b[0m,   │  \u001b[38;5;34m2,369,024\u001b[0m │ coord_conv2d_17[\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mCoordConv2D\u001b[0m)       │ \u001b[38;5;34m512\u001b[0m), (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m,  │            │ coord_conv2d_17[\u001b[38;5;34m…\u001b[0m │\n",
       "│                     │ \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m2\u001b[0m)]           │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ coord_conv2d_19     │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m30\u001b[0m,   │  \u001b[38;5;34m2,369,024\u001b[0m │ coord_conv2d_18[\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mCoordConv2D\u001b[0m)       │ \u001b[38;5;34m512\u001b[0m), (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m,  │            │ coord_conv2d_18[\u001b[38;5;34m…\u001b[0m │\n",
       "│                     │ \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m2\u001b[0m)]           │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling2d_14    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m15\u001b[0m,     │          \u001b[38;5;34m0\u001b[0m │ coord_conv2d_19[\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ \u001b[38;5;34m512\u001b[0m)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_7 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m15\u001b[0m,     │          \u001b[38;5;34m0\u001b[0m │ max_pooling2d_14… │\n",
       "│                     │ \u001b[38;5;34m512\u001b[0m)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_22 (\u001b[38;5;33mConv2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m15\u001b[0m,     │    \u001b[38;5;34m262,656\u001b[0m │ dropout_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│                     │ \u001b[38;5;34m512\u001b[0m)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_23 (\u001b[38;5;33mConv2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m15\u001b[0m,     │    \u001b[38;5;34m131,328\u001b[0m │ conv2d_22[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│                     │ \u001b[38;5;34m256\u001b[0m)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ heatmap (\u001b[38;5;33mConv2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m1\u001b[0m)  │        \u001b[38;5;34m257\u001b[0m │ conv2d_23[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_max_pooling… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ heatmap[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "│ (\u001b[38;5;33mGlobalMaxPooling2…\u001b[0m │                   │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">8,084,225</span> (30.84 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m8,084,225\u001b[0m (30.84 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">8,084,225</span> (30.84 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m8,084,225\u001b[0m (30.84 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1a005a-757f-4d9a-b4ca-e29426bdefad",
   "metadata": {},
   "source": [
    "Again, as mentioned above, of note here is the learning rate chosen, which is 10^-4.\n",
    "\n",
    "The original version of this notebook attempted to use 10^-3, but that caused the model to fail to converge after just one epoch of training (later epochs failed to make any progress). In comparison, using 10^-4 (while slower initially) seems to work well in training the data over the course of at least four epochs.\n",
    "\n",
    "As noted in the previous notebook, we might consider decaying the learning rate even further (due to our small batch size) later in this training for later epochs, because the small batch size can cause noisiness in gradient descent updates (so dampening those updates a bit can help the model converge). We'll monitor and see how that goes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c56071c5-1a70-4a5b-8fc9-e736ebde6a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = keras.optimizers.Adam(learning_rate=1e-4)\n",
    "loss = keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "model.compile(loss=loss, optimizer=opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1381c59e-18b7-4b16-ac29-74d4b30ad806",
   "metadata": {},
   "source": [
    "We create a checkpoint saving function just to deal with iPython's many kernel crashes.\n",
    "After each pass through the data and all batches, we'll save the weights, reload the model, and keep going.\n",
    "\n",
    "We'll save the weights in `checkpoints/epoch_{number_of_epoch}.weights.h5`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fea1e22b-bc9c-4ed7-807c-baa52e71d908",
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkpoint_creator(checkpoint_path):\n",
    "    # saving the model's weights in case the iPython kernel crashes (which it likes to do)\n",
    "    cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                       save_weights_only=True,\n",
    "                                       verbose=1)\n",
    "    return cp_callback\n",
    "\n",
    "def checkpoint_loader(checkpoint_path, model):\n",
    "    model.load_weights(checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2112eb5-6cd4-45fe-8d21-83ef2074fc4d",
   "metadata": {},
   "source": [
    "First pass through the data. Again, with each pass, we set it up with:\n",
    "- A batch_size of 64 (to fit the data into GPU memory)\n",
    "- Shuffle the data with a random_state (we pick a new seed each time, but record that seed here so we don't reuse it in a later epoch)\n",
    "- Set up a new checkpoint for each epoch (in case any one epoch of training crashes/fails)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "25ddfa68-e121-4440-bacc-125c7f9f00fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed = make_tf_with_flips_loader(data_root = TORNET_DATA_INPUT_FOLDER, \n",
    "                                         data_type = \"train\", # or 'test'\n",
    "                                         years = list(range(2013, 2023)),\n",
    "                                         batch_size = 64, \n",
    "                                         weights = None,\n",
    "                                         include_az = False,\n",
    "                                         random_state = 5678,\n",
    "                                         select_keys = ALL_VARIABLES + [\"coordinates\", \"range_folded_mask\"],\n",
    "                                         tilt_last = True,\n",
    "                                         from_tfds = False,\n",
    "                                         tfds_data_version =\"1.1.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d79f207a-8752-4bc2-992b-054c1af63860",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1731973690.907013   17376 service.cc:148] XLA service 0x7f62f8019090 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1731973690.907543   17376 service.cc:156]   StreamExecutor device (0): NVIDIA GeForce RTX 3070, Compute Capability 8.6\n",
      "2024-11-18 15:48:11.049961: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1731973691.398709   17376 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
      "E0000 00:00:1731973695.090274   17376 gpu_timer.cc:82] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "I0000 00:00:1731973715.098554   17376 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   2867/Unknown \u001b[1m9308s\u001b[0m 3s/step - loss: 1.3145\n",
      "Epoch 1: saving model to checkpoints/epoch_flipping_1_1e-4.weights.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 18:23:15.488076: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "2024-11-18 18:23:15.490720: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "\t [[IteratorGetNext/_12]]\n",
      "2024-11-18 18:23:15.491729: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 18317437428072606055\n",
      "2024-11-18 18:23:15.491761: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 12506195242215264983\n",
      "2024-11-18 18:23:15.491768: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 944059850056747535\n",
      "2024-11-18 18:23:15.491772: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 1264007328355077277\n",
      "2024-11-18 18:23:15.491958: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 4262657239284205003\n",
      "2024-11-18 18:23:15.492109: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 14515781602841935606\n",
      "2024-11-18 18:23:15.492124: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 293195287202640562\n",
      "2024-11-18 18:23:15.492129: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 2149321471284453066\n",
      "2024-11-18 18:23:15.492173: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 6842426328757759826\n",
      "/usr/lib/python3.9/contextlib.py:137: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2867/2867\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9310s\u001b[0m 3s/step - loss: 1.3143\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7f640bab3490>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(preprocessed, epochs=1, callbacks=[checkpoint_creator(\"checkpoints/epoch_flipping_1_1e-4.weights.h5\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da684670-9f21-4229-b50f-dcccc27b9342",
   "metadata": {},
   "source": [
    "Now we loop through a bunch of epochs to train the CNN further, making sure to change the random_state each time to mini-batch the data differently each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6e5125-b437-41f2-9111-3a285ece926a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   2867/Unknown \u001b[1m9357s\u001b[0m 3s/step - loss: 0.3107\n",
      "Epoch 1: saving model to checkpoints/epoch_flipping_2_1e-4.weights.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 23:22:16.192384: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "\t [[IteratorGetNext/_12]]\n",
      "2024-11-18 23:22:16.197645: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 18317437428072606055\n",
      "2024-11-18 23:22:16.197671: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 12506195242215264983\n",
      "2024-11-18 23:22:16.197679: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 944059850056747535\n",
      "2024-11-18 23:22:16.197684: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 1264007328355077277\n",
      "2024-11-18 23:22:16.197716: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 4262657239284205003\n",
      "2024-11-18 23:22:16.197746: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 14515781602841935606\n",
      "2024-11-18 23:22:16.197752: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 293195287202640562\n",
      "2024-11-18 23:22:16.197756: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 2149321471284453066\n",
      "2024-11-18 23:22:16.197794: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 6842426328757759826\n",
      "/usr/lib/python3.9/contextlib.py:137: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2867/2867\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9359s\u001b[0m 3s/step - loss: 0.3107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-19 01:40:10.174991: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 117964864 bytes after encountering the first element of size 117964864 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      3/Unknown \u001b[1m34s\u001b[0m 14s/step - loss: 0.2262"
     ]
    }
   ],
   "source": [
    "for epoch_num in range(2, 5):\n",
    "    resampled = make_tf_with_flips_loader(data_root = TORNET_DATA_INPUT_FOLDER, \n",
    "                                          data_type = \"train\", # or 'test'\n",
    "                                          years = list(range(2013, 2023)),\n",
    "                                          batch_size = 64, \n",
    "                                          weights = None,\n",
    "                                          include_az = False,\n",
    "                                          random_state = 1234 + epoch_num,\n",
    "                                          select_keys = ALL_VARIABLES + [\"coordinates\", \"range_folded_mask\"],\n",
    "                                          tilt_last = True,\n",
    "                                          from_tfds = False,\n",
    "                                          tfds_data_version =\"1.1.0\")\n",
    "    checkpoint_loader(\"checkpoints/epoch_flipping_{}_1e-4.weights.h5\".format(str(epoch_num - 1)), model)\n",
    "    model.fit(resampled, epochs=1, callbacks=[checkpoint_creator(\"checkpoints/epoch_flipping_{}_1e-4.weights.h5\".format(epoch_num))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3fcb6332-6578-4146-830b-73013695004c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/keras/src/saving/saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 54 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1732045742.183686   21628 service.cc:148] XLA service 0x7f6628018f50 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1732045742.184072   21628 service.cc:156]   StreamExecutor device (0): NVIDIA GeForce RTX 3070, Compute Capability 8.6\n",
      "2024-11-19 11:49:02.310836: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1732045742.629618   21628 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
      "I0000 00:00:1732045765.958023   21628 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   2867/Unknown \u001b[1m9839s\u001b[0m 3s/step - loss: 0.2538\n",
      "Epoch 1: saving model to checkpoints/epoch_flipping_3_1e-4.weights.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-19 14:32:58.471917: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "2024-11-19 14:32:58.472487: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "\t [[IteratorGetNext/_18]]\n",
      "2024-11-19 14:32:58.472792: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 18125960373766345697\n",
      "2024-11-19 14:32:58.472815: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 9072318686438974239\n",
      "2024-11-19 14:32:58.472821: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 1040842881928324157\n",
      "2024-11-19 14:32:58.472825: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 1237740458513806647\n",
      "2024-11-19 14:32:58.472829: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 12632295518641251943\n",
      "2024-11-19 14:32:58.472834: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 9048254918056089224\n",
      "2024-11-19 14:32:58.472836: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 1534801658280426416\n",
      "2024-11-19 14:32:58.472839: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 18221563437090840100\n",
      "2024-11-19 14:32:58.472873: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 12621612113631110748\n",
      "/usr/lib/python3.9/contextlib.py:137: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2867/2867\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9842s\u001b[0m 3s/step - loss: 0.2538\n",
      "   2867/Unknown \u001b[1m11148s\u001b[0m 4s/step - loss: 0.2376\n",
      "Epoch 1: saving model to checkpoints/epoch_flipping_4_1e-4.weights.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-19 19:56:04.202585: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "\t [[IteratorGetNext/_18]]\n",
      "/usr/lib/python3.9/contextlib.py:137: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n",
      "2024-11-19 19:56:04.206253: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 18125960373766345697\n",
      "2024-11-19 19:56:04.206311: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 9072318686438974239\n",
      "2024-11-19 19:56:04.206324: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 1040842881928324157\n",
      "2024-11-19 19:56:04.206328: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 1237740458513806647\n",
      "2024-11-19 19:56:04.206332: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 12632295518641251943\n",
      "2024-11-19 19:56:04.206356: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 9048254918056089224\n",
      "2024-11-19 19:56:04.206361: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 1534801658280426416\n",
      "2024-11-19 19:56:04.206364: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 18221563437090840100\n",
      "2024-11-19 19:56:04.206389: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 12621612113631110748\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2867/2867\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11151s\u001b[0m 4s/step - loss: 0.2376\n"
     ]
    }
   ],
   "source": [
    "for epoch_num in range(3, 5):\n",
    "    resampled = make_tf_with_flips_loader(data_root = TORNET_DATA_INPUT_FOLDER, \n",
    "                                          data_type = \"train\", # or 'test'\n",
    "                                          years = list(range(2013, 2023)),\n",
    "                                          batch_size = 64, \n",
    "                                          weights = None,\n",
    "                                          include_az = False,\n",
    "                                          random_state = 1234 + epoch_num,\n",
    "                                          select_keys = ALL_VARIABLES + [\"coordinates\", \"range_folded_mask\"],\n",
    "                                          tilt_last = True,\n",
    "                                          from_tfds = False,\n",
    "                                          tfds_data_version =\"1.1.0\")\n",
    "    checkpoint_loader(\"checkpoints/epoch_flipping_{}_1e-4.weights.h5\".format(str(epoch_num - 1)), model)\n",
    "    model.fit(resampled, epochs=1, callbacks=[checkpoint_creator(\"checkpoints/epoch_flipping_{}_1e-4.weights.h5\".format(epoch_num))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7273478-035d-4ef9-809d-31389e6eebef",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch_num in range(5, 7):\n",
    "    resampled = make_tf_with_flips_loader(data_root = TORNET_DATA_INPUT_FOLDER, \n",
    "                                          data_type = \"train\", # or 'test'\n",
    "                                          years = list(range(2013, 2023)),\n",
    "                                          batch_size = 64, \n",
    "                                          weights = None,\n",
    "                                          include_az = False,\n",
    "                                          random_state = 1234 + epoch_num,\n",
    "                                          select_keys = ALL_VARIABLES + [\"coordinates\", \"range_folded_mask\"],\n",
    "                                          tilt_last = True,\n",
    "                                          from_tfds = False,\n",
    "                                          tfds_data_version =\"1.1.0\")\n",
    "    checkpoint_loader(\"checkpoints/epoch_flipping_{}_1e-4.weights.h5\".format(str(epoch_num - 1)), model)\n",
    "    model.fit(resampled, epochs=1, callbacks=[checkpoint_creator(\"checkpoints/epoch_flipping_{}_1e-4.weights.h5\".format(epoch_num))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17fda516-715f-4019-98e6-abaafc4914c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/keras/src/saving/saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 54 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   2867/Unknown \u001b[1m9105s\u001b[0m 3s/step - loss: 0.2140\n",
      "Epoch 1: saving model to checkpoints/epoch_flipping_7_1e-4.weights.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-20 20:50:42.094627: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "\t [[IteratorGetNext/_8]]\n",
      "2024-11-20 20:50:42.097826: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 11792009927715927949\n",
      "2024-11-20 20:50:42.097844: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 10503351137423630075\n",
      "2024-11-20 20:50:42.097849: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 8873016923517038427\n",
      "2024-11-20 20:50:42.097855: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 1358983975771822302\n",
      "2024-11-20 20:50:42.097859: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 5465932250389205152\n",
      "2024-11-20 20:50:42.097862: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 3497044298265619036\n",
      "2024-11-20 20:50:42.097865: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 12000807716464649026\n",
      "2024-11-20 20:50:42.097867: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 14565796626948094974\n",
      "2024-11-20 20:50:42.097897: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 6796921093029644112\n",
      "/usr/lib/python3.9/contextlib.py:137: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2867/2867\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9107s\u001b[0m 3s/step - loss: 0.2140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-20 23:03:57.895288: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 117964864 bytes after encountering the first element of size 117964864 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      4/Unknown \u001b[1m18s\u001b[0m 4s/step - loss: 0.1937"
     ]
    }
   ],
   "source": [
    "for epoch_num in range(7, 8):\n",
    "    resampled = make_tf_with_flips_loader(data_root = TORNET_DATA_INPUT_FOLDER, \n",
    "                                          data_type = \"train\", # or 'test'\n",
    "                                          years = list(range(2013, 2023)),\n",
    "                                          batch_size = 64, \n",
    "                                          weights = None,\n",
    "                                          include_az = False,\n",
    "                                          random_state = 1234 + epoch_num,\n",
    "                                          select_keys = ALL_VARIABLES + [\"coordinates\", \"range_folded_mask\"],\n",
    "                                          tilt_last = True,\n",
    "                                          from_tfds = False,\n",
    "                                          tfds_data_version =\"1.1.0\")\n",
    "    checkpoint_loader(\"checkpoints/epoch_flipping_{}_1e-4.weights.h5\".format(str(epoch_num - 1)), model)\n",
    "    model.fit(resampled, epochs=1, callbacks=[checkpoint_creator(\"checkpoints/epoch_flipping_{}_1e-4.weights.h5\".format(epoch_num))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e1ac9482-dfbb-4103-972d-0be373ac1c88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/keras/src/saving/saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 54 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1732181742.860156   15124 service.cc:148] XLA service 0x7f3e9c018eb0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1732181742.860471   15124 service.cc:156]   StreamExecutor device (0): NVIDIA GeForce RTX 3070, Compute Capability 8.6\n",
      "2024-11-21 01:35:42.973705: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1732181743.275769   15124 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
      "E0000 00:00:1732181745.413249   15124 gpu_timer.cc:82] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "I0000 00:00:1732181766.645622   15124 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   2867/Unknown \u001b[1m9033s\u001b[0m 3s/step - loss: 0.2079\n",
      "Epoch 1: saving model to checkpoints/epoch_flipping_8_1e-4.weights.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-21 04:06:12.691764: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "2024-11-21 04:06:12.691856: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "\t [[IteratorGetNext/_10]]\n",
      "2024-11-21 04:06:12.691868: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 12827997819925368895\n",
      "2024-11-21 04:06:12.691871: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 10259843889643680281\n",
      "2024-11-21 04:06:12.691876: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 2283971990208613307\n",
      "2024-11-21 04:06:12.691879: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 65729284537371621\n",
      "2024-11-21 04:06:12.691885: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 4687782750711658532\n",
      "2024-11-21 04:06:12.691887: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 9391880592736711200\n",
      "2024-11-21 04:06:12.691890: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 5774203960222149612\n",
      "2024-11-21 04:06:12.691894: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 391584738457161164\n",
      "2024-11-21 04:06:12.691918: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 590530956098035478\n",
      "/usr/lib/python3.9/contextlib.py:137: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2867/2867\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9035s\u001b[0m 3s/step - loss: 0.2079\n"
     ]
    }
   ],
   "source": [
    "for epoch_num in range(8, 9):\n",
    "    resampled = make_tf_with_flips_loader(data_root = TORNET_DATA_INPUT_FOLDER, \n",
    "                                          data_type = \"train\", # or 'test'\n",
    "                                          years = list(range(2013, 2023)),\n",
    "                                          batch_size = 64, \n",
    "                                          weights = None,\n",
    "                                          include_az = False,\n",
    "                                          random_state = 1234 + epoch_num,\n",
    "                                          select_keys = ALL_VARIABLES + [\"coordinates\", \"range_folded_mask\"],\n",
    "                                          tilt_last = True,\n",
    "                                          from_tfds = False,\n",
    "                                          tfds_data_version =\"1.1.0\")\n",
    "    checkpoint_loader(\"checkpoints/epoch_flipping_{}_1e-4.weights.h5\".format(str(epoch_num - 1)), model)\n",
    "    model.fit(resampled, epochs=1, callbacks=[checkpoint_creator(\"checkpoints/epoch_flipping_{}_1e-4.weights.h5\".format(epoch_num))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d536baeb-a12e-481e-b9cc-8221b190d8f9",
   "metadata": {},
   "source": [
    "We checked the validation AUC's as we went along in order to make sure that AUC was increasing with each epoch of training,\n",
    "and to make adjustments to hyperparameters if we were stalling out at any point.\n",
    "\n",
    "After epochs 7 and 8, we noticed that there wasn't much of an improvement from epoch 7 to epoch 8 in the validation AUC.\n",
    "Hence, we theorized that there might be a learning rate issue (maybe the learning rate is too large to make improvements),\n",
    "so we reduced the learning rate from 10^-4 to 5 * 10^-5, cutting the learning rate in half and hoping that would help\n",
    "as the loss begins to converge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "805c8e95-dbf4-4c52-94e5-b217e00141a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/keras/src/saving/saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 54 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   2867/Unknown \u001b[1m14114s\u001b[0m 5s/step - loss: 0.1976"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-21 15:08:15.938919: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "\t [[IteratorGetNext/_10]]\n",
      "2024-11-21 15:08:15.942789: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 12827997819925368895\n",
      "2024-11-21 15:08:15.942815: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 10259843889643680281\n",
      "2024-11-21 15:08:15.942823: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 2283971990208613307\n",
      "2024-11-21 15:08:15.942828: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 65729284537371621\n",
      "2024-11-21 15:08:15.942839: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 4687782750711658532\n",
      "2024-11-21 15:08:15.942843: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 9391880592736711200\n",
      "2024-11-21 15:08:15.942847: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 5774203960222149612\n",
      "2024-11-21 15:08:15.942850: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 391584738457161164\n",
      "2024-11-21 15:08:15.942898: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 590530956098035478\n",
      "/usr/lib/python3.9/contextlib.py:137: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: saving model to checkpoints/epoch_flipping_9_1e-4.weights.h5\n",
      "\u001b[1m2867/2867\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14135s\u001b[0m 5s/step - loss: 0.1976\n"
     ]
    }
   ],
   "source": [
    "opt = keras.optimizers.Adam(learning_rate=5e-5)\n",
    "loss = keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "model.compile(loss=loss, optimizer=opt)\n",
    "\n",
    "for epoch_num in range(9, 10):\n",
    "    resampled = make_tf_with_flips_loader(data_root = TORNET_DATA_INPUT_FOLDER, \n",
    "                                          data_type = \"train\", # or 'test'\n",
    "                                          years = list(range(2013, 2023)),\n",
    "                                          batch_size = 64, \n",
    "                                          weights = None,\n",
    "                                          include_az = False,\n",
    "                                          random_state = 1234 + epoch_num,\n",
    "                                          select_keys = ALL_VARIABLES + [\"coordinates\", \"range_folded_mask\"],\n",
    "                                          tilt_last = True,\n",
    "                                          from_tfds = False,\n",
    "                                          tfds_data_version =\"1.1.0\")\n",
    "    checkpoint_loader(\"checkpoints/epoch_flipping_{}_1e-4.weights.h5\".format(str(epoch_num - 1)), model)\n",
    "    model.fit(resampled, epochs=1, callbacks=[checkpoint_creator(\"checkpoints/epoch_flipping_{}_1e-4.weights.h5\".format(epoch_num))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92fb03ba-5628-4248-8e5d-1e6a31423265",
   "metadata": {},
   "source": [
    "## Testing on test data\n",
    "\n",
    "In our test data, we DON'T want to do any flipping or data augmentation. So, before we run the test data \n",
    "evaluation, we have to bring back TorNet's original functions for loading the test data into a \n",
    "Tensorflow dataset.\n",
    "\n",
    "After we do that, we can run through the testing data for each epoch to see how the model performed\n",
    "after each epoch of training in terms of AUC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d5925fa-1c91-4ec1-b5b7-e24f87d23843",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tf_dataset(files:str,\n",
    "                      variables: List[str]=ALL_VARIABLES,\n",
    "                      n_frames:int=1,\n",
    "                      tilt_last: bool=True) -> tf.data.Dataset:\n",
    "    \"\"\"\n",
    "    This is Tornet's main function for loading the data from the folder where it's all stored.\n",
    "    \n",
    "    As they stated, this creates a TF dataset object via the function read_file (which reads the NetCDF files\n",
    "    into the data one at a time).\n",
    "    \"\"\"\n",
    "    assert len(files)>0\n",
    "    # grab one file to gets keys, shapes, etc\n",
    "    data = read_file(files[0],variables=variables,n_frames=n_frames, tilt_last=tilt_last)\n",
    "    \n",
    "    output_signature = { k:tf.TensorSpec(shape=data[k].shape,dtype=data[k].dtype,name=k) for k in data }\n",
    "    def gen():\n",
    "        for f in files:\n",
    "            yield read_file(f,variables=variables,n_frames=n_frames, tilt_last=tilt_last)\n",
    "    ds = tf.data.Dataset.from_generator(gen,\n",
    "                                        output_signature=output_signature)\n",
    "    return ds\n",
    "\n",
    "def make_tf_loader(data_root: str, \n",
    "            data_type:str='train', # or 'test'\n",
    "            years: list=list(range(2013,2023)),\n",
    "            batch_size: int=128, \n",
    "            weights: Dict=None,\n",
    "            include_az: bool=False,\n",
    "            random_state:int=1234,\n",
    "            select_keys: list=None,\n",
    "            tilt_last: bool=True,\n",
    "            from_tfds: bool=False,\n",
    "            tfds_data_version: str='1.1.0',\n",
    "            num_epochs: int=3):\n",
    "    \"\"\"\n",
    "    This TorNet library function is used to load the data into Tensorflow.\n",
    "    We're going to use the `create_tf_dataset` function from above, \n",
    "    then we'll use `preproc` to preprocess it.\n",
    "\n",
    "    One important note - we tried a bunch of different functions for shuffling \n",
    "    and batching, repeating the dataset, etc. to try to be able to run\n",
    "    many epochs with one function call. It wasn't working.\n",
    "    Even the `drop_remainder=True` that we've added here to ds.batch\n",
    "    seems to not really have an effect, as the model training\n",
    "    still throws an error at the end of training about running out of data.\n",
    "    \"\"\"\n",
    "    \n",
    "    if from_tfds: # fast loader\n",
    "        import tensorflow_datasets as tfds\n",
    "        import tornet.data.tfds.tornet.tornet_dataset_builder # registers 'tornet'\n",
    "        ds = tfds.load('tornet:%s' % tfds_data_version ,split='+'.join(['%s-%d' % (data_type,y) for y in years]))\n",
    "        # Assumes data was saved with tilt_last=True and converts it to tilt_last=False\n",
    "        if not tilt_last:\n",
    "            ds = ds.map(lambda d: pp.permute_dims(d,(0,3,1,2), backend=tf))\n",
    "    else: # Load directly from netcdf files\n",
    "        file_list = query_catalog(data_root, data_type, years, random_state)\n",
    "        ds = create_tf_dataset(file_list,variables=ALL_VARIABLES,n_frames=1, tilt_last=tilt_last) \n",
    "\n",
    "    ds = preproc(ds,weights,include_az,select_keys,tilt_last)\n",
    "    ds = ds.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    # this has been adjusted to include drop_remainder=True\n",
    "    ds = ds.batch(batch_size, drop_remainder=True)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83a410b-d4c1-4179-8bef-a48d523191a5",
   "metadata": {},
   "source": [
    "For each epoch that we trained through, let's test the model and see how it did after each pass of the data.\n",
    "AUC is our evaluation metric here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5bed535-c6bc-48b3-a510-3a0db1369894",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/keras/src/saving/saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 54 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1732079977.799323     856 service.cc:148] XLA service 0x7ff4a800db50 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1732079977.799664     856 service.cc:156]   StreamExecutor device (0): NVIDIA GeForce RTX 3070, Compute Capability 8.6\n",
      "2024-11-19 21:19:37.834266: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1732079977.932640     856 cuda_dnn.cc:529] Loaded cuDNN version 90300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      2/Unknown \u001b[1m13s\u001b[0m 67ms/step - AUC: 0.4362 - loss: 0.1598   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1732079986.378947     856 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m491/491\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1590s\u001b[0m 3s/step - AUC: 0.8184 - loss: 0.2154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-19 21:46:03.694408: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "2024-11-19 21:46:03.694462: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "\t [[IteratorGetNext/_14]]\n",
      "2024-11-19 21:46:03.694472: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 12744976677381798467\n",
      "2024-11-19 21:46:03.694475: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 9096861446359773735\n",
      "2024-11-19 21:46:03.694479: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 5502915226632464905\n",
      "2024-11-19 21:46:03.694482: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 8169583992736786955\n",
      "2024-11-19 21:46:03.694485: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 17117769626377841927\n",
      "2024-11-19 21:46:03.694488: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 992421454627991131\n",
      "2024-11-19 21:46:03.694490: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 9799722824554614607\n",
      "2024-11-19 21:46:03.694494: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 8612676659198615726\n",
      "2024-11-19 21:46:03.694513: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 11735222185637296740\n",
      "/usr/lib/python3.9/contextlib.py:137: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n",
      "/usr/local/lib/python3.9/dist-packages/keras/src/saving/saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'rmsprop', because it has 2 variables whereas the saved optimizer has 54 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m491/491\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1668s\u001b[0m 3s/step - AUC: 0.8388 - loss: 0.2028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-19 22:13:52.101356: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "\t [[IteratorGetNext/_14]]\n",
      "/usr/lib/python3.9/contextlib.py:137: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n",
      "2024-11-19 22:13:52.101397: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 9799722824554614607\n",
      "2024-11-19 22:13:52.101403: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 9096861446359773735\n",
      "2024-11-19 22:13:52.101407: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 8169583992736786955\n",
      "2024-11-19 22:13:52.101410: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 5502915226632464905\n",
      "2024-11-19 22:13:52.101413: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 12744976677381798467\n",
      "2024-11-19 22:13:52.101416: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 17117769626377841927\n",
      "2024-11-19 22:13:52.101419: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 992421454627991131\n",
      "2024-11-19 22:13:52.101425: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 8612676659198615726\n",
      "2024-11-19 22:13:52.101442: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 11735222185637296740\n"
     ]
    }
   ],
   "source": [
    "test_data = make_tf_loader(data_root = TORNET_DATA_INPUT_FOLDER, \n",
    "                              data_type = \"test\",\n",
    "                              years = list(range(2013, 2023)),\n",
    "                              batch_size = 64, \n",
    "                              weights = None,\n",
    "                              include_az = False,\n",
    "                              random_state = 5678,\n",
    "                              select_keys = ALL_VARIABLES + [\"coordinates\", \"range_folded_mask\"],\n",
    "                              tilt_last = True,\n",
    "                              from_tfds = False,\n",
    "                              tfds_data_version =\"1.1.0\")\n",
    "\n",
    "for epoch_num in range(3, 5):\n",
    "    metrics = [keras.metrics.AUC(from_logits=True,name='AUC')]\n",
    "    checkpoint_loader(\"checkpoints/epoch_flipping_{}_1e-4.weights.h5\".format(str(epoch_num)), model)\n",
    "    model.compile(loss=loss, metrics=metrics)\n",
    "    model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f6beef51-759a-44aa-900d-41eac512f31c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/keras/src/saving/saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 54 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1732135689.176521    4987 service.cc:148] XLA service 0x7fae94003af0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1732135689.176917    4987 service.cc:156]   StreamExecutor device (0): NVIDIA GeForce RTX 3070, Compute Capability 8.6\n",
      "2024-11-20 12:48:09.210472: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1732135689.302695    4987 cuda_dnn.cc:529] Loaded cuDNN version 90300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      2/Unknown \u001b[1m13s\u001b[0m 74ms/step - AUC: 0.4285 - loss: 0.1316   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1732135697.837069    4987 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m491/491\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1533s\u001b[0m 3s/step - AUC: 0.8609 - loss: 0.1914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-20 13:13:38.874242: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "2024-11-20 13:13:38.874306: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "\t [[IteratorGetNext/_8]]\n",
      "2024-11-20 13:13:38.874324: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 8873016923517038427\n",
      "2024-11-20 13:13:38.874329: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 10503351137423630075\n",
      "2024-11-20 13:13:38.874334: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 11792009927715927949\n",
      "2024-11-20 13:13:38.874339: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 1358983975771822302\n",
      "2024-11-20 13:13:38.874342: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 5465932250389205152\n",
      "2024-11-20 13:13:38.874345: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 3497044298265619036\n",
      "2024-11-20 13:13:38.874349: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 14565796626948094974\n",
      "2024-11-20 13:13:38.874351: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 12000807716464649026\n",
      "2024-11-20 13:13:38.874371: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 6796921093029644112\n",
      "/usr/lib/python3.9/contextlib.py:137: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] Unable to synchronously open file (unable to open file: name = 'checkpoints/epoch_flipping_7_1e-4.weights.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch_num \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m6\u001b[39m, \u001b[38;5;241m8\u001b[39m):\n\u001b[1;32m     14\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m [keras\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mAUC(from_logits\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAUC\u001b[39m\u001b[38;5;124m'\u001b[39m)]\n\u001b[0;32m---> 15\u001b[0m     \u001b[43mcheckpoint_loader\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcheckpoints/epoch_flipping_\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m_1e-4.weights.h5\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mepoch_num\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m     model\u001b[38;5;241m.\u001b[39mcompile(loss\u001b[38;5;241m=\u001b[39mloss, metrics\u001b[38;5;241m=\u001b[39mmetrics)\n\u001b[1;32m     17\u001b[0m     model\u001b[38;5;241m.\u001b[39mevaluate(test_data)\n",
      "Cell \u001b[0;32mIn[9], line 9\u001b[0m, in \u001b[0;36mcheckpoint_loader\u001b[0;34m(checkpoint_path, model)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheckpoint_loader\u001b[39m(checkpoint_path, model):\n\u001b[0;32m----> 9\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/h5py/_hl/files.py:561\u001b[0m, in \u001b[0;36mFile.__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, alignment_threshold, alignment_interval, meta_block_size, **kwds)\u001b[0m\n\u001b[1;32m    552\u001b[0m     fapl \u001b[38;5;241m=\u001b[39m make_fapl(driver, libver, rdcc_nslots, rdcc_nbytes, rdcc_w0,\n\u001b[1;32m    553\u001b[0m                      locking, page_buf_size, min_meta_keep, min_raw_keep,\n\u001b[1;32m    554\u001b[0m                      alignment_threshold\u001b[38;5;241m=\u001b[39malignment_threshold,\n\u001b[1;32m    555\u001b[0m                      alignment_interval\u001b[38;5;241m=\u001b[39malignment_interval,\n\u001b[1;32m    556\u001b[0m                      meta_block_size\u001b[38;5;241m=\u001b[39mmeta_block_size,\n\u001b[1;32m    557\u001b[0m                      \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    558\u001b[0m     fcpl \u001b[38;5;241m=\u001b[39m make_fcpl(track_order\u001b[38;5;241m=\u001b[39mtrack_order, fs_strategy\u001b[38;5;241m=\u001b[39mfs_strategy,\n\u001b[1;32m    559\u001b[0m                      fs_persist\u001b[38;5;241m=\u001b[39mfs_persist, fs_threshold\u001b[38;5;241m=\u001b[39mfs_threshold,\n\u001b[1;32m    560\u001b[0m                      fs_page_size\u001b[38;5;241m=\u001b[39mfs_page_size)\n\u001b[0;32m--> 561\u001b[0m     fid \u001b[38;5;241m=\u001b[39m make_fid(name, mode, userblock_size, fapl, fcpl, swmr\u001b[38;5;241m=\u001b[39mswmr)\n\u001b[1;32m    563\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(libver, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    564\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_libver \u001b[38;5;241m=\u001b[39m libver\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/h5py/_hl/files.py:235\u001b[0m, in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m swmr \u001b[38;5;129;01mand\u001b[39;00m swmr_support:\n\u001b[1;32m    234\u001b[0m         flags \u001b[38;5;241m|\u001b[39m\u001b[38;5;241m=\u001b[39m h5f\u001b[38;5;241m.\u001b[39mACC_SWMR_READ\n\u001b[0;32m--> 235\u001b[0m     fid \u001b[38;5;241m=\u001b[39m \u001b[43mh5f\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfapl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfapl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr+\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    237\u001b[0m     fid \u001b[38;5;241m=\u001b[39m h5f\u001b[38;5;241m.\u001b[39mopen(name, h5f\u001b[38;5;241m.\u001b[39mACC_RDWR, fapl\u001b[38;5;241m=\u001b[39mfapl)\n",
      "File \u001b[0;32mh5py/_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/h5f.pyx:102\u001b[0m, in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] Unable to synchronously open file (unable to open file: name = 'checkpoints/epoch_flipping_7_1e-4.weights.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
     ]
    }
   ],
   "source": [
    "test_data = make_tf_loader(data_root = TORNET_DATA_INPUT_FOLDER, \n",
    "                              data_type = \"test\",\n",
    "                              years = list(range(2013, 2023)),\n",
    "                              batch_size = 64, \n",
    "                              weights = None,\n",
    "                              include_az = False,\n",
    "                              random_state = 5678,\n",
    "                              select_keys = ALL_VARIABLES + [\"coordinates\", \"range_folded_mask\"],\n",
    "                              tilt_last = True,\n",
    "                              from_tfds = False,\n",
    "                              tfds_data_version =\"1.1.0\")\n",
    "\n",
    "for epoch_num in range(6, 8):\n",
    "    metrics = [keras.metrics.AUC(from_logits=True,name='AUC')]\n",
    "    checkpoint_loader(\"checkpoints/epoch_flipping_{}_1e-4.weights.h5\".format(str(epoch_num)), model)\n",
    "    model.compile(loss=loss, metrics=metrics)\n",
    "    model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b23d658c-f74c-4691-b8ab-871464321593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m491/491\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1607s\u001b[0m 3s/step - AUC: 0.8659 - loss: 0.1903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-21 08:35:31.749291: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "\t [[IteratorGetNext/_10]]\n",
      "2024-11-21 08:35:31.750965: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 12827997819925368895\n",
      "2024-11-21 08:35:31.750981: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 10259843889643680281\n",
      "2024-11-21 08:35:31.750989: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 2283971990208613307\n",
      "2024-11-21 08:35:31.750994: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 65729284537371621\n",
      "2024-11-21 08:35:31.751002: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 4687782750711658532\n",
      "2024-11-21 08:35:31.751006: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 9391880592736711200\n",
      "2024-11-21 08:35:31.751010: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 5774203960222149612\n",
      "2024-11-21 08:35:31.751015: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 391584738457161164\n",
      "2024-11-21 08:35:31.751036: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 590530956098035478\n",
      "/usr/lib/python3.9/contextlib.py:137: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n",
      "/usr/local/lib/python3.9/dist-packages/keras/src/saving/saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'rmsprop', because it has 2 variables whereas the saved optimizer has 54 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m491/491\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1610s\u001b[0m 3s/step - AUC: 0.8663 - loss: 0.1890\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-21 09:02:22.190477: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 12827997819925368895\n",
      "/usr/lib/python3.9/contextlib.py:137: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n",
      "2024-11-21 09:02:22.190519: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 10259843889643680281\n",
      "2024-11-21 09:02:22.190526: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 2283971990208613307\n",
      "2024-11-21 09:02:22.190532: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 65729284537371621\n",
      "2024-11-21 09:02:22.190539: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 4687782750711658532\n",
      "2024-11-21 09:02:22.190545: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 9391880592736711200\n",
      "2024-11-21 09:02:22.190549: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 5774203960222149612\n",
      "2024-11-21 09:02:22.190553: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 391584738457161164\n",
      "2024-11-21 09:02:22.190574: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 590530956098035478\n"
     ]
    }
   ],
   "source": [
    "test_data = make_tf_loader(data_root = TORNET_DATA_INPUT_FOLDER, \n",
    "                              data_type = \"test\",\n",
    "                              years = list(range(2013, 2023)),\n",
    "                              batch_size = 64, \n",
    "                              weights = None,\n",
    "                              include_az = False,\n",
    "                              random_state = 5678,\n",
    "                              select_keys = ALL_VARIABLES + [\"coordinates\", \"range_folded_mask\"],\n",
    "                              tilt_last = True,\n",
    "                              from_tfds = False,\n",
    "                              tfds_data_version =\"1.1.0\")\n",
    "\n",
    "for epoch_num in range(7, 9):\n",
    "    metrics = [keras.metrics.AUC(from_logits=True,name='AUC')]\n",
    "    checkpoint_loader(\"checkpoints/epoch_flipping_{}_1e-4.weights.h5\".format(str(epoch_num)), model)\n",
    "    model.compile(loss=loss, metrics=metrics)\n",
    "    model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d837a5f-6755-4a69-bf33-59c0ca3857c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/keras/src/saving/saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 54 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1732231767.728496   23418 service.cc:148] XLA service 0x7fce5000d3c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1732231767.729281   23418 service.cc:156]   StreamExecutor device (0): NVIDIA GeForce RTX 3070, Compute Capability 8.6\n",
      "2024-11-21 15:29:27.773098: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1732231767.896476   23418 cuda_dnn.cc:529] Loaded cuDNN version 90300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      2/Unknown \u001b[1m11s\u001b[0m 65ms/step - AUC: 0.4374 - loss: 0.1248   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1732231774.914547   23418 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m491/491\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1546s\u001b[0m 3s/step - AUC: 0.8804 - loss: 0.1836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-21 15:55:09.686421: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "2024-11-21 15:55:09.686505: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "\t [[IteratorGetNext/_14]]\n",
      "2024-11-21 15:55:09.686522: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 13487443787624175879\n",
      "2024-11-21 15:55:09.686527: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 6682284121449466355\n",
      "2024-11-21 15:55:09.686532: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 6907914624901376875\n",
      "2024-11-21 15:55:09.686535: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 17021916078633971215\n",
      "2024-11-21 15:55:09.686538: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 16962612165641987899\n",
      "2024-11-21 15:55:09.686542: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 12872416329011749254\n",
      "2024-11-21 15:55:09.686546: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 2794086980411018186\n",
      "2024-11-21 15:55:09.686549: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 15686286726541339666\n",
      "2024-11-21 15:55:09.686570: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 3241818016139244394\n",
      "/usr/lib/python3.9/contextlib.py:137: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n"
     ]
    }
   ],
   "source": [
    "test_data = make_tf_loader(data_root = TORNET_DATA_INPUT_FOLDER, \n",
    "                              data_type = \"test\",\n",
    "                              years = list(range(2013, 2023)),\n",
    "                              batch_size = 64, \n",
    "                              weights = None,\n",
    "                              include_az = False,\n",
    "                              random_state = 5678,\n",
    "                              select_keys = ALL_VARIABLES + [\"coordinates\", \"range_folded_mask\"],\n",
    "                              tilt_last = True,\n",
    "                              from_tfds = False,\n",
    "                              tfds_data_version =\"1.1.0\")\n",
    "\n",
    "for epoch_num in range(9, 10):\n",
    "    metrics = [keras.metrics.AUC(from_logits=True,name='AUC')]\n",
    "    checkpoint_loader(\"checkpoints/epoch_flipping_{}_1e-4.weights.h5\".format(str(epoch_num)), model)\n",
    "    model.compile(loss=loss, metrics=metrics)\n",
    "    model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0068a4-724f-4694-a82e-6b32c164e704",
   "metadata": {},
   "source": [
    "## Significant results!\n",
    "\n",
    "This above result (of a validation AUC of .880) is already massive. By using our augmented dataset to train the data (and give it more tornadic examples), we've increased the performance of the TorNet model without any error analysis on the validation set (basically, \"without peeking\").\n",
    "\n",
    "Note that none of the test/validation examples are flipped. Only our training examples included flipped tornadoes. This means that even though the flipped tornadoes aren't necessarily \"real\" tornadoes, flipped tornadoes are good enough examples to help the CNN architecture learn what a tornado looks like!\n",
    "\n",
    "Our intuition that the class imbalance was causing issues seems to have been validated - even without five-fold cross validation, the model we've trained with the augmented dataset has a better AUC than the original CNN model that was used. With a validation AUC of .8804, we've beaten the benchmark that TorNet set with their CNN (which, when published, had achieved a validation AUC of .874 at its best). \n",
    "\n",
    "The idea to also decrease the learning rate for the last couple of iterations also seems to have helped here. Between epochs 7 and 8 of training (with a learning rate of 10^-4), we saw hardly any improvement in the AUC. So, we used a learning rate of 5 * 10^-5 for epoch 9 of training, and got a massive AUC improvement (probably due to dampening the noisy updates now that we're \"closer\" to converging).\n",
    "\n",
    "The best part is that there's still a chance for further improvement here with more training, which we'll attempt below. We might even consider further reducing the learning rate if we don't see any improvements on these next epochs of training, just to see if we can squeeze out any more improvements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e534d3-6bad-4692-8047-c5ec63ae92d2",
   "metadata": {},
   "source": [
    "## Running further epochs on the data\n",
    "\n",
    "Let's run more epochs with this adjusted learning rate and test the model's performance on the test validation set after each epoch.\n",
    "\n",
    "Our plan is to continue using decaying learning rate and early stopping in order to get the best model possible without overfitting or overtuning - once we see that validation AUC is not improving, we decrease learning rate, and if we see that even a decreased learning rate doesn't improve the performance, we stop training more epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5dfed196-9256-47af-a0fc-8271be8f13f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/keras/src/saving/saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 54 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   2867/Unknown \u001b[1m9457s\u001b[0m 3s/step - loss: 0.1893\n",
      "Epoch 1: saving model to checkpoints/epoch_flipping_10_1e-5.weights.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-22 13:23:46.313413: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 9826376891565193259\n",
      "2024-11-22 13:23:46.315518: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 13217425807873929449\n",
      "2024-11-22 13:23:46.315531: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 9430844678276428555\n",
      "2024-11-22 13:23:46.315536: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 10349365694754166267\n",
      "2024-11-22 13:23:46.315692: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 13923780728266016189\n",
      "2024-11-22 13:23:46.315715: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 6329571622297899138\n",
      "2024-11-22 13:23:46.315721: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 7972913564882484030\n",
      "2024-11-22 13:23:46.315730: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 216873167555570550\n",
      "2024-11-22 13:23:46.315757: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 311535426139881232\n",
      "/usr/lib/python3.9/contextlib.py:137: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2867/2867\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9459s\u001b[0m 3s/step - loss: 0.1893\n"
     ]
    }
   ],
   "source": [
    "opt = keras.optimizers.Adam(learning_rate=1e-5)\n",
    "loss = keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "model.compile(loss=loss, optimizer=opt)\n",
    "\n",
    "for epoch_num in range(10, 11):\n",
    "    resampled = make_tf_with_flips_loader(data_root = TORNET_DATA_INPUT_FOLDER, \n",
    "                                          data_type = \"train\", # or 'test'\n",
    "                                          years = list(range(2013, 2023)),\n",
    "                                          batch_size = 64, \n",
    "                                          weights = None,\n",
    "                                          include_az = False,\n",
    "                                          random_state = 1234 + epoch_num,\n",
    "                                          select_keys = ALL_VARIABLES + [\"coordinates\", \"range_folded_mask\"],\n",
    "                                          tilt_last = True,\n",
    "                                          from_tfds = False,\n",
    "                                          tfds_data_version =\"1.1.0\")\n",
    "    checkpoint_loader(\"checkpoints/epoch_flipping_{}_1e-4.weights.h5\".format(str(epoch_num - 1)), model)\n",
    "    model.fit(resampled, epochs=1, callbacks=[checkpoint_creator(\"checkpoints/epoch_flipping_{}_1e-5.weights.h5\".format(epoch_num))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6815005d-0ad5-4e89-9311-346441538208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m491/491\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1540s\u001b[0m 3s/step - AUC: 0.8781 - loss: 0.1824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.9/contextlib.py:137: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n",
      "2024-11-22 08:22:17.536217: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "\t [[IteratorGetNext/_4]]\n",
      "2024-11-22 08:22:17.536269: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 9826376891565193259\n",
      "2024-11-22 08:22:17.536275: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 13217425807873929449\n",
      "2024-11-22 08:22:17.536280: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 9430844678276428555\n",
      "2024-11-22 08:22:17.536284: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 10349365694754166267\n",
      "2024-11-22 08:22:17.536287: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 13923780728266016189\n",
      "2024-11-22 08:22:17.536293: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 6329571622297899138\n",
      "2024-11-22 08:22:17.536296: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 7972913564882484030\n",
      "2024-11-22 08:22:17.536299: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 216873167555570550\n",
      "2024-11-22 08:22:17.536319: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 311535426139881232\n"
     ]
    }
   ],
   "source": [
    "test_data = make_tf_loader(data_root = TORNET_DATA_INPUT_FOLDER, \n",
    "                              data_type = \"test\",\n",
    "                              years = list(range(2013, 2023)),\n",
    "                              batch_size = 64, \n",
    "                              weights = None,\n",
    "                              include_az = False,\n",
    "                              random_state = 5678,\n",
    "                              select_keys = ALL_VARIABLES + [\"coordinates\", \"range_folded_mask\"],\n",
    "                              tilt_last = True,\n",
    "                              from_tfds = False,\n",
    "                              tfds_data_version =\"1.1.0\")\n",
    "\n",
    "for epoch_num in range(10, 11):\n",
    "    metrics = [keras.metrics.AUC(from_logits=True,name='AUC')]\n",
    "    checkpoint_loader(\"checkpoints/epoch_flipping_{}_1e-4.weights.h5\".format(str(epoch_num)), model)\n",
    "    model.compile(loss=loss, metrics=metrics)\n",
    "    model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0026d250-976e-4a2e-ae07-9b2c2fb723f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m491/491\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1711s\u001b[0m 3s/step - AUC: 0.8821 - loss: 0.1819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-22 14:00:35.525271: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "\t [[IteratorGetNext/_4]]\n",
      "2024-11-22 14:00:35.526461: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 9826376891565193259\n",
      "2024-11-22 14:00:35.526473: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 13217425807873929449\n",
      "2024-11-22 14:00:35.526478: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 9430844678276428555\n",
      "2024-11-22 14:00:35.526482: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 10349365694754166267\n",
      "2024-11-22 14:00:35.526485: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 13923780728266016189\n",
      "2024-11-22 14:00:35.526493: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 6329571622297899138\n",
      "2024-11-22 14:00:35.52/usr/lib/python3.9/contextlib.py:137: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n",
      "6497: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 216873167555570550\n",
      "2024-11-22 14:00:35.526500: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 7972913564882484030\n",
      "2024-11-22 14:00:35.526522: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 311535426139881232\n"
     ]
    }
   ],
   "source": [
    "test_data = make_tf_loader(data_root = TORNET_DATA_INPUT_FOLDER, \n",
    "                              data_type = \"test\",\n",
    "                              years = list(range(2013, 2023)),\n",
    "                              batch_size = 64, \n",
    "                              weights = None,\n",
    "                              include_az = False,\n",
    "                              random_state = 5678,\n",
    "                              select_keys = ALL_VARIABLES + [\"coordinates\", \"range_folded_mask\"],\n",
    "                              tilt_last = True,\n",
    "                              from_tfds = False,\n",
    "                              tfds_data_version =\"1.1.0\")\n",
    "\n",
    "for epoch_num in range(10, 11):\n",
    "    metrics = [keras.metrics.AUC(from_logits=True,name='AUC')]\n",
    "    checkpoint_loader(\"checkpoints/epoch_flipping_{}_1e-5.weights.h5\".format(str(epoch_num)), model)\n",
    "    model.compile(loss=loss, metrics=metrics)\n",
    "    model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36f9da5-6515-4f4f-8c2b-dc3be9e1dba2",
   "metadata": {},
   "source": [
    "Now we have achieved an AUC of .8821 (with different weights, and still better than the TorNet performance!). Is it possible to improve the model even further?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "842ce6bc-b4ff-4351-95a6-74fa96f1ef9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/keras/src/saving/saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 54 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1732354833.996844     164 service.cc:148] XLA service 0x7f49f80190a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1732354833.997136     164 service.cc:156]   StreamExecutor device (0): NVIDIA GeForce RTX 3070, Compute Capability 8.6\n",
      "2024-11-23 01:40:34.112307: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1732354834.426541     164 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
      "E0000 00:00:1732354836.374730     164 gpu_timer.cc:82] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "I0000 00:00:1732354859.903105     164 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   2867/Unknown \u001b[1m9069s\u001b[0m 3s/step - loss: 0.1880\n",
      "Epoch 1: saving model to checkpoints/epoch_flipping_11_1e-5.weights.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-23 04:11:39.798536: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "2024-11-23 04:11:39.798572: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "\t [[IteratorGetNext/_6]]\n",
      "2024-11-23 04:11:39.798582: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 130575521149143211\n",
      "2024-11-23 04:11:39.798586: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 12751758492268379335\n",
      "2024-11-23 04:11:39.798591: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 13236180624781059707\n",
      "2024-11-23 04:11:39.798596: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 18308500856403318264\n",
      "2024-11-23 04:11:39.798599: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 14579245408181732044\n",
      "2024-11-23 04:11:39.798602: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 14547304850208728136\n",
      "2024-11-23 04:11:39.798605: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 11502475179987952946\n",
      "2024-11-23 04:11:39.798608: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 8644507059846065498\n",
      "2024-11-23 04:11:39.798638: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 8553275239263700834\n",
      "/usr/lib/python3.9/contextlib.py:137: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2867/2867\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9071s\u001b[0m 3s/step - loss: 0.1880\n"
     ]
    }
   ],
   "source": [
    "opt = keras.optimizers.Adam(learning_rate=1e-5)\n",
    "loss = keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "model.compile(loss=loss, optimizer=opt)\n",
    "\n",
    "for epoch_num in range(11, 12):\n",
    "    resampled = make_tf_with_flips_loader(data_root = TORNET_DATA_INPUT_FOLDER, \n",
    "                                          data_type = \"train\", # or 'test'\n",
    "                                          years = list(range(2013, 2023)),\n",
    "                                          batch_size = 64, \n",
    "                                          weights = None,\n",
    "                                          include_az = False,\n",
    "                                          random_state = 1234 + epoch_num,\n",
    "                                          select_keys = ALL_VARIABLES + [\"coordinates\", \"range_folded_mask\"],\n",
    "                                          tilt_last = True,\n",
    "                                          from_tfds = False,\n",
    "                                          tfds_data_version =\"1.1.0\")\n",
    "    checkpoint_loader(\"checkpoints/epoch_flipping_{}_1e-5.weights.h5\".format(str(epoch_num - 1)), model)\n",
    "    model.fit(resampled, epochs=1, callbacks=[checkpoint_creator(\"checkpoints/epoch_flipping_{}_1e-5.weights.h5\".format(epoch_num))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "28078cde-fd3e-4db7-a8bf-a44e8eeac0ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m491/491\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1555s\u001b[0m 3s/step - AUC: 0.8846 - loss: 0.1804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.9/contextlib.py:137: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n",
      "2024-11-23 10:07:47.012139: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "\t [[IteratorGetNext/_10]]\n",
      "2024-11-23 10:07:47.012567: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 130575521149143211\n",
      "2024-11-23 10:07:47.012575: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 12751758492268379335\n",
      "2024-11-23 10:07:47.012580: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 13236180624781059707\n",
      "2024-11-23 10:07:47.012587: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 8644507059846065498\n",
      "2024-11-23 10:07:47.012589: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 14579245408181732044\n",
      "2024-11-23 10:07:47.012593: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 14547304850208728136\n",
      "2024-11-23 10:07:47.012595: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 18308500856403318264\n",
      "2024-11-23 10:07:47.012598: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 11502475179987952946\n",
      "2024-11-23 10:07:47.012620: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 8553275239263700834\n"
     ]
    }
   ],
   "source": [
    "test_data = make_tf_loader(data_root = TORNET_DATA_INPUT_FOLDER, \n",
    "                              data_type = \"test\",\n",
    "                              years = list(range(2013, 2023)),\n",
    "                              batch_size = 64, \n",
    "                              weights = None,\n",
    "                              include_az = False,\n",
    "                              random_state = 5678,\n",
    "                              select_keys = ALL_VARIABLES + [\"coordinates\", \"range_folded_mask\"],\n",
    "                              tilt_last = True,\n",
    "                              from_tfds = False,\n",
    "                              tfds_data_version =\"1.1.0\")\n",
    "\n",
    "for epoch_num in range(11, 12):\n",
    "    metrics = [keras.metrics.AUC(from_logits=True,name='AUC')]\n",
    "    checkpoint_loader(\"checkpoints/epoch_flipping_{}_1e-5.weights.h5\".format(str(epoch_num)), model)\n",
    "    model.compile(loss=loss, metrics=metrics)\n",
    "    model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f891db93-b81a-410a-94b5-ad2829f73dd6",
   "metadata": {},
   "source": [
    ".8846 AUC is even better. We've now beaten the original AUC of the baseline CNN by .010."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fded5157-6e54-456a-b928-8be70cee11e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/keras/src/saving/saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 54 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   2867/Unknown \u001b[1m9405s\u001b[0m 3s/step - loss: 0.1860\n",
      "Epoch 1: saving model to checkpoints/epoch_flipping_12_1e-5.weights.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-23 15:06:40.504714: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 130575521149143211\n",
      "2024-11-23 15:06:40.507329: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 12751758492268379335\n",
      "2024-11-23 15:06:40.507346: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 13236180624781059707\n",
      "2024-11-23 15:06:40.507370: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 8644507059846065498\n",
      "2024-11-23 15:06:40.507377: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 14579245408181732044\n",
      "2024-11-23 15:06:40.507512: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 14547304850208728136\n",
      "2024-11-23 15:06:40.507524: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 11502475179987952946\n",
      "2024-11-23 15:06:40.507529: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 18308500856403318264\n",
      "2024-11-23 15:06:40.507570: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 8553275239263700834\n",
      "/usr/lib/python3.9/contextlib.py:137: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2867/2867\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9408s\u001b[0m 3s/step - loss: 0.1860\n"
     ]
    }
   ],
   "source": [
    "opt = keras.optimizers.Adam(learning_rate=1e-5)\n",
    "loss = keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "model.compile(loss=loss, optimizer=opt)\n",
    "\n",
    "for epoch_num in range(12, 13):\n",
    "    resampled = make_tf_with_flips_loader(data_root = TORNET_DATA_INPUT_FOLDER, \n",
    "                                          data_type = \"train\", # or 'test'\n",
    "                                          years = list(range(2013, 2023)),\n",
    "                                          batch_size = 64, \n",
    "                                          weights = None,\n",
    "                                          include_az = False,\n",
    "                                          random_state = 1234 + epoch_num,\n",
    "                                          select_keys = ALL_VARIABLES + [\"coordinates\", \"range_folded_mask\"],\n",
    "                                          tilt_last = True,\n",
    "                                          from_tfds = False,\n",
    "                                          tfds_data_version =\"1.1.0\")\n",
    "    checkpoint_loader(\"checkpoints/epoch_flipping_{}_1e-5.weights.h5\".format(str(epoch_num - 1)), model)\n",
    "    model.fit(resampled, epochs=1, callbacks=[checkpoint_creator(\"checkpoints/epoch_flipping_{}_1e-5.weights.h5\".format(epoch_num))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "27915272-8403-447f-89b3-5777774b9c9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m491/491\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1626s\u001b[0m 3s/step - AUC: 0.8854 - loss: 0.1806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-23 15:37:57.456046: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "\t [[IteratorGetNext/_10]]\n",
      "2024-11-23 15:37:57.458470: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 130575521149143211\n",
      "2024-11-23 15:37:57.458483: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 12751758492268379335\n",
      "2024-11-23 15:37:57.458489: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 13236180624781059707\n",
      "2024-11-23 15:37:57.458498: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 8644507059846065498\n",
      "2024-11-23 15:37:57.458502: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 14579245408181732044\n",
      "2024-11-23 15:37:57.458506: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 14547304850208728136\n",
      "2024-11-23 15:37:57.458510: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 11502475179987952946\n",
      "2024-11-23 15:37:57.458514: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 18308500856403318264\n",
      "2024-11-23 15:37:57.458542: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 8553275239263700834\n",
      "/usr/lib/python3.9/contextlib.py:137: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n"
     ]
    }
   ],
   "source": [
    "test_data = make_tf_loader(data_root = TORNET_DATA_INPUT_FOLDER, \n",
    "                              data_type = \"test\",\n",
    "                              years = list(range(2013, 2023)),\n",
    "                              batch_size = 64, \n",
    "                              weights = None,\n",
    "                              include_az = False,\n",
    "                              random_state = 5678,\n",
    "                              select_keys = ALL_VARIABLES + [\"coordinates\", \"range_folded_mask\"],\n",
    "                              tilt_last = True,\n",
    "                              from_tfds = False,\n",
    "                              tfds_data_version =\"1.1.0\")\n",
    "\n",
    "for epoch_num in range(12, 13):\n",
    "    metrics = [keras.metrics.AUC(from_logits=True,name='AUC')]\n",
    "    checkpoint_loader(\"checkpoints/epoch_flipping_{}_1e-5.weights.h5\".format(str(epoch_num)), model)\n",
    "    model.compile(loss=loss, metrics=metrics)\n",
    "    model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee228101-c1bb-4a7d-9a40-7feea2af4383",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Ok, we're probably in a reasonable place to stop given that validation loss has stopped decreasing even with our decayed learning rate. \n",
    "With early stopping as our guideline (so that we don't end up overfitting), we should mark this as our best validation AUC.\n",
    "\n",
    "We observed high validation AUC's for several epochs along the training (which we could, in some sense, consider to be separate models) with different sets of weights which all beat the original .8742 benchmark - epochs 9, 10, 11, and 12 all had validation AUC's that were .878 or better in the validation AUC. This is strong evidence that there are a number of possible trained models (using the augmented dataset) that are better than the original model trained on just the original dataset.\n",
    "\n",
    "What does this mean for us? There are a few points to add here as important notes:\n",
    "- First, flipping the tornadoes to create augmented data did NOT adversely impact the model that was trained. This was a concern that we had initially (is there weird \"directionality\" or \"spin direction\" that is lost when we flip the tornado?) but it proved to not be the case, and we achieved great success here.\n",
    "- In fact, flipping the tornado images to create augmented data (as mentioned above) is probably a great strategy to 'create' more tornadoes and help correct for class imbalance caused by a relative lack of tornado data.\n",
    "- In addition, what this shows above all is that CNN models trained on the dataset could do even better if we collect more tornado examples. Future research should not only use mirroring to augment the dataset, but should also focus on expanding the dataset to include more examples of labeled tornadoes. If anything, while the success of this solution shows that flipping some data is a good stopgap measure given a lack of data, it also proves that just generally having more tornado examples would absolutely help boost performance (because our synthetic tornadoes are essentially nothing more than just additional examples of tornadoes that help correct class imbalance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2146a7c-00e2-4fc2-a154-3bee058549f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
